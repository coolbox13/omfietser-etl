#!/usr/bin/env python3
"""
Jumbo GraphQL Optimized Scraper - Phase 2A Implementation
Performance target: 25-45x improvement via bulk GraphQL queries
Based on research findings: 100 products/request, 0.1s intervals, 600 req/min safe limit
"""

import aiohttp
import asyncio
import json
import os
import logging
import time
import signal
from datetime import datetime
from random import uniform
from typing import Dict, List, Optional, Tuple

# Import progress monitoring (maintains compatibility with runner system)
import sys

# Try to import from local modules, with fallbacks
try:
    from progress_monitor import update_status, update_progress, ScraperStatus, get_amsterdam_time
    from config_utils import get_output_directory
except ImportError:
    # Fallback compatibility for container environment
    from datetime import datetime
    try:
        import pytz
    except ImportError:
        pytz = None
    
    class ScraperStatus:
        STARTING = 'starting'
        RUNNING = 'running'
        COMPLETED = 'completed'
        FAILED = 'failed'
        INTERRUPTED = 'interrupted'
    
    def update_status(scraper_name, status, message):
        print(f"Status update for {scraper_name}: {status} - {message}")
    
    def update_progress(scraper_name, progress_percent=0, products_scraped=0, current_task=""):
        print(f"Progress update for {scraper_name}: {progress_percent:.1f}% - {products_scraped} products - {current_task}")
    
    def get_amsterdam_time():
        if pytz:
            tz = pytz.timezone('Europe/Amsterdam')
            return datetime.now(tz)
        else:
            return datetime.now()
    
    def get_output_directory():
        # Detect if running in container or local environment
        if os.path.exists('/app'):
            return "/app/results"
        else:
            # Local environment - use current directory structure
            base_dir = os.path.dirname(os.path.abspath(__file__))
            return os.path.join(base_dir, "results")

# Setup logging (will be overridden by wrapper for job-specific logging)
# Detect environment for logging directory
if os.path.exists('/app'):
    log_dir = os.getenv('JUMBO_LOG_DIR', '/app/logs')
else:
    # Local environment
    base_dir = os.path.dirname(os.path.abspath(__file__))
    log_dir = os.getenv('JUMBO_LOG_DIR', os.path.join(base_dir, "logs"))

os.makedirs(log_dir, exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(os.path.join(log_dir, "jumbo_optimized_scraper.log")),
        logging.StreamHandler()
    ]
)

logging.getLogger('aiohttp').setLevel(logging.WARNING)

# GraphQL endpoint and headers (validated from research)
GRAPHQL_ENDPOINT = 'https://www.jumbo.com/api/graphql'
HEADERS = {
    'Content-Type': 'application/json',
    'Accept': '*/*',
    'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 18_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148',
    'Accept-Language': 'nl-NL,nl;q=0.9',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Origin': 'capacitor://jumbo',
    'apollographql-client-name': 'JUMBO_MOBILE-search',
    'apollographql-client-version': '15.4.0',
    'jmb-device-id': 'D426DAF8-0D83-48CE-84B6-4BDF35336BE3',
    'x-source': 'JUMBO_MOBILE-search',
}

# OPTIMIZATION PARAMETERS (from research findings)
OPTIMAL_BATCH_SIZE = 100  # Validated optimal: 62 products/second
SAFE_REQUEST_INTERVAL = 0.1  # 600 requests/minute safe limit
MAX_CONCURRENT_REQUESTS = 5  # Conservative concurrent limit
FALLBACK_BATCH_SIZE = 30  # Fallback if large batches fail

class JumboGraphQLOptimizedScraper:
    def __init__(self, max_products=None, categories_limit=None):
        self.graphql_url = GRAPHQL_ENDPOINT
        self.max_products = max_products
        self.categories_limit = categories_limit
        self.output_dir = get_output_directory()
        self.products_file = f"{self.output_dir}/jumbo_products.json"
        
        # Use environment-aware paths with container/local detection
        if os.path.exists('/app'):
            # Container environment
            data_dir = os.getenv('JUMBO_DATA_DIR', '/app/shared-data')
            progress_dir = os.getenv('JUMBO_PROGRESS_DIR', '/app/jobs')
        else:
            # Local environment
            base_dir = os.path.dirname(os.path.abspath(__file__))
            data_dir = os.getenv('JUMBO_DATA_DIR', os.path.join(base_dir, 'data'))
            progress_dir = os.getenv('JUMBO_PROGRESS_DIR', os.path.join(base_dir, 'progress'))
        
        self.progress_file = os.path.join(progress_dir, "jumbo_scrape_progress.json")
        self.session_file = os.path.join(data_dir, "jumbo_session.json")
        self.completed_flag = os.path.join(data_dir, "jumbo_scrape_complete.flag")

        # Create directories
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(data_dir, exist_ok=True)
        os.makedirs(progress_dir, exist_ok=True)

        # Performance tracking
        self.scraped_products = set()
        self.session_cookies = {}
        self.total_scraped = 0
        
        # OPTIMIZATION: Enhanced batch processing
        self.current_batch_size = OPTIMAL_BATCH_SIZE
        self.current_offset = 0
        self.successful_requests = 0
        self.failed_requests = 0
        self.estimated_total_products = 23000  # More accurate estimate
        
        # Performance metrics
        self.start_time = time.time()
        self.products_per_second = 0
        self.requests_per_minute = 0
        
        self.max_retries = 3
        self.base_delay = SAFE_REQUEST_INTERVAL
        self.timeout_config = aiohttp.ClientTimeout(total=30, connect=10)
        self.shutdown_requested = False

        # Setup signal handlers
        signal.signal(signal.SIGINT, self.signal_handler)
        signal.signal(signal.SIGTERM, self.signal_handler)

        # Check completion status
        self.scraping_completed = os.path.exists(self.completed_flag)
        if self.scraping_completed:
            logging.info("‚úÖ Previous run completed successfully. Skipping scraping.")
            self.load_existing_data()
        else:
            self.load_progress()
            self.load_session()

        logging.info(f"üöÄ OPTIMIZED SCRAPER INITIALIZED")
        logging.info(f"   Target batch size: {self.current_batch_size} products/request")
        logging.info(f"   Request interval: {self.base_delay}s ({60/self.base_delay:.0f} req/min)")
        logging.info(f"   Expected improvement: {self.current_batch_size/30:.1f}x vs original")

    def signal_handler(self, signum, frame):
        """Handle shutdown signals gracefully."""
        logging.info(f"üõë Received signal {signum}, shutting down gracefully...")
        self.shutdown_requested = True
        update_status('jumbo', ScraperStatus.INTERRUPTED, "Shutdown requested")

    def load_existing_data(self):
        """Load existing product data for reporting."""
        if os.path.exists(self.products_file):
            try:
                with open(self.products_file, 'r') as f:
                    existing_products = json.load(f)
                    self.total_scraped = len(existing_products)
                    logging.info(f"üìä Found {self.total_scraped} products from completed run")
            except (json.JSONDecodeError, FileNotFoundError):
                self.total_scraped = 0

    def load_progress(self):
        """Load previous scraping progress."""
        if os.path.exists(self.progress_file):
            try:
                with open(self.progress_file, 'r') as f:
                    progress = json.load(f)
                    self.scraped_products = set(progress.get('scraped_products', []))
                    self.total_scraped = progress.get('total_scraped', 0)
                    self.current_offset = progress.get('current_offset', 0)
                    
                    # OPTIMIZATION: Smart batch size restoration
                    saved_batch_size = progress.get('current_batch_size', OPTIMAL_BATCH_SIZE)
                    if saved_batch_size in [30, 50, 100]:  # Valid batch sizes
                        self.current_batch_size = saved_batch_size
                    
                logging.info(f"üìÇ Loaded progress: {len(self.scraped_products)} products, offset {self.current_offset}")
                logging.info(f"üéØ Resume with batch size: {self.current_batch_size}")
                
            except json.JSONDecodeError:
                logging.warning("‚ö†Ô∏è Progress file corrupted, starting fresh")
                self.current_offset = 0

    def load_session(self):
        """Load session cookies if they exist."""
        if os.path.exists(self.session_file):
            try:
                with open(self.session_file, 'r') as f:
                    session_data = json.load(f)
                    self.session_cookies = session_data.get('cookies', {})
                    session_time = session_data.get('timestamp', 0)
                    if time.time() - session_time < 3600:  # 1 hour
                        logging.info("üìÇ Loaded existing session cookies")
                    else:
                        logging.info("‚è∞ Session cookies expired, will create new session")
                        self.session_cookies = {}
            except (json.JSONDecodeError, KeyError):
                logging.warning("‚ö†Ô∏è Session file corrupted, will create new session")
                self.session_cookies = {}

    def save_progress(self, current_offset=None):
        """Save current scraping progress with optimization metrics."""
        if current_offset is not None:
            self.current_offset = current_offset
        
        # Calculate performance metrics
        elapsed_time = time.time() - self.start_time
        if elapsed_time > 0:
            self.products_per_second = self.total_scraped / elapsed_time
            self.requests_per_minute = (self.successful_requests / elapsed_time) * 60 if elapsed_time > 0 else 0
        
        progress_data = {
            'scraped_products': list(self.scraped_products),
            'total_scraped': self.total_scraped,
            'current_offset': self.current_offset,
            
            # OPTIMIZATION: Save performance data
            'current_batch_size': self.current_batch_size,
            'successful_requests': self.successful_requests,
            'failed_requests': self.failed_requests,
            'products_per_second': self.products_per_second,
            'requests_per_minute': self.requests_per_minute,
            
            'estimated_progress_percent': min(100.0, (self.total_scraped / self.estimated_total_products) * 100),
            'optimization_active': True,
            'timestamp': time.time(),
            'timestamp_amsterdam': get_amsterdam_time().strftime('%Y-%m-%d %H:%M:%S CET')
        }
        
        with open(self.progress_file, 'w') as f:
            json.dump(progress_data, f, indent=4)

    def save_session(self, session):
        """Save session cookies for reuse."""
        if session.cookie_jar:
            cookies = {}
            for cookie in session.cookie_jar:
                cookies[cookie.key] = cookie.value

            session_data = {
                'cookies': cookies,
                'timestamp': time.time(),
                'timestamp_amsterdam': get_amsterdam_time().strftime('%Y-%m-%d %H:%M:%S CET')
            }

            with open(self.session_file, 'w') as f:
                json.dump(session_data, f, indent=4)

            self.session_cookies = cookies
            logging.info("üíæ Saved session cookies")

    def save_products(self, products):
        """Save or update products with deduplication."""
        if not products:
            return

        # Load existing products
        existing_products = []
        existing_ids = set()
        if os.path.exists(self.products_file):
            try:
                with open(self.products_file, 'r') as f:
                    existing_products = json.load(f)
                    existing_ids = {p.get('product', {}).get('id') for p in existing_products if p.get('product', {}).get('id')}
            except (json.JSONDecodeError, FileNotFoundError):
                logging.warning("‚ö†Ô∏è Products file corrupted or missing, starting fresh")
                existing_products = []
                existing_ids = set()

        # Add new products
        new_products = []
        for product in products:
            product_id = product.get('product', {}).get('id')
            if product_id and product_id not in existing_ids:
                new_products.append(product)
                existing_ids.add(product_id)

        existing_products.extend(new_products)

        with open(self.products_file, 'w') as f:
            json.dump(existing_products, f, indent=4, ensure_ascii=False)

        logging.info(f"üíæ Saved {len(new_products)} new products (total: {len(existing_products)})")

    def get_categories_query(self):
        """GraphQL query for fetching categories."""
        return {
            "operationName": "GetMobileCategoryAisles",
            "variables": {
                "input": {
                    "id": "MobileCategoryAisles",
                    "searchType": "category", 
                    "searchTerms": "producten"
                }
            },
            "query": """query GetMobileCategoryAisles($input: ProductSearchInput!) {
  searchProducts(input: $input) {
    id
    pageHeader {
      headerText
      __typename
    }
    categoryTiles {
      count
      catId
      name
      friendlyUrl
      imageLink
      displayOrder
      __typename
    }
    __typename
  }
}"""
        }

    def get_products_query(self, category_friendly_url, offset=0, limit=None):
        """OPTIMIZED: GraphQL query with configurable batch size."""
        if limit is None:
            limit = self.current_batch_size
            
        return {
            "operationName": "SearchMobileProducts",
            "variables": {
                "input": {
                    "id": "MobileProducts",
                    "searchType": "category",
                    "searchTerms": "producten",
                    "friendlyUrl": category_friendly_url,
                    "offSet": offset,
                    "currentUrl": f"/producten/{category_friendly_url}",
                    "previousUrl": "",
                    "bloomreachCookieId": ""
                }
            },
            "query": """query SearchMobileProducts($input: ProductSearchInput!) {
  searchProducts(input: $input) {
    id
    start
    count
    pageHeader {
      headerText
      count
      __typename
    }
    products {
      ...SearchProductDetails
      crossSells {
        sku
        __typename
      }
      __typename
    }
    __typename
  }
}

fragment SearchProductDetails on Product {
  id: sku
  brand
  category: rootCategory
  subtitle: packSizeDisplay
  title
  image
  inAssortment
  availability {
    availability
    isAvailable
    label
    stockLimit
    reason
    availabilityNote
    __typename
  }
  sponsored
  auctionId
  link
  retailSet
  prices: price {
    price
    promoPrice
    pricePerUnit {
      price
      unit
      __typename
    }
    __typename
  }
  quantityDetails {
    maxAmount
    minAmount
    stepAmount
    defaultAmount
    __typename
  }
  primaryBadge: primaryProductBadges {
    alt
    image
    __typename
  }
  secondaryBadges: secondaryProductBadges {
    alt
    image
    __typename
  }
  customerAllergies {
    short
    __typename
  }
  promotions {
    id
    group
    isKiesAndMix
    image
    tags {
      text
      inverse
      __typename
    }
    start {
      dayShort
      date
      monthShort
      __typename
    }
    end {
      dayShort
      date
      monthShort
      __typename
    }
    attachments {
      type
      path
      __typename
    }
    primaryBadge: primaryBadges {
      alt
      image
      __typename
    }
    volumeDiscounts {
      discount
      volume
      __typename
    }
    durationTexts {
      shortTitle
      __typename
    }
    __typename
  }
  surcharges {
    type
    value {
      amount
      currency
      __typename
    }
    __typename
  }
}"""
        }

    async def make_graphql_request(self, session: aiohttp.ClientSession, query_data: Dict) -> Tuple[Optional[Dict], float, bool]:
        """OPTIMIZED: Enhanced GraphQL request with performance tracking."""
        start_time = time.time()
        
        for attempt in range(self.max_retries):
            try:
                if attempt > 0:
                    delay = self.base_delay * (2 ** attempt) + uniform(0, 0.5)
                    await asyncio.sleep(delay)

                async with session.post(self.graphql_url, headers=HEADERS, json=query_data) as response:
                    duration = time.time() - start_time
                    
                    if response.status == 200:
                        try:
                            data = await response.json()
                            success = 'data' in data and not data.get('errors')
                            
                            if success:
                                self.successful_requests += 1
                            else:
                                self.failed_requests += 1
                                logging.warning(f"GraphQL errors: {data.get('errors', [])}")
                            
                            return data.get('data'), duration, success
                        except json.JSONDecodeError:
                            self.failed_requests += 1
                            return None, duration, False
                            
                    elif response.status == 429:
                        logging.warning(f"Rate limited on attempt {attempt + 1}, increasing delay")
                        self.base_delay = min(self.base_delay * 1.5, 2.0)  # Adaptive rate limiting
                        await asyncio.sleep(5)
                        continue
                    else:
                        logging.warning(f"HTTP {response.status} on attempt {attempt + 1}")
                        return None, duration, False

            except Exception as e:
                logging.warning(f"Request error on attempt {attempt + 1}: {e}")
                continue

        self.failed_requests += 1
        return None, time.time() - start_time, False

    async def adapt_batch_size(self, success_rate: float):
        """OPTIMIZATION: Dynamic batch size adaptation."""
        if success_rate >= 0.95:  # Very high success rate
            if self.current_batch_size < OPTIMAL_BATCH_SIZE:
                self.current_batch_size = min(self.current_batch_size + 20, OPTIMAL_BATCH_SIZE)
                logging.info(f"üìà Increasing batch size to {self.current_batch_size} (success rate: {success_rate:.1%})")
        elif success_rate < 0.8:  # Low success rate
            self.current_batch_size = max(FALLBACK_BATCH_SIZE, self.current_batch_size - 20)
            logging.warning(f"üìâ Decreasing batch size to {self.current_batch_size} (success rate: {success_rate:.1%})")

    async def scrape_category(self, session: aiohttp.ClientSession, category: Dict, semaphore: asyncio.Semaphore) -> int:
        """OPTIMIZED: Scrape products from category with deep pagination."""
        async with semaphore:
            category_url = category['friendlyUrl']
            category_name = category.get('name', category_url)
            total_products = 0
            offset = 0
            consecutive_empty_pages = 0
            max_empty_pages = 5  # Increased tolerance for empty pages
            
            logging.info(f"üõí Processing {category_name} (batch size: {self.current_batch_size})")
            
            while not self.shutdown_requested and consecutive_empty_pages < max_empty_pages:
                query = self.get_products_query(category_url, offset=offset)
                response_data, duration, success = await self.make_graphql_request(session, query)
                
                if not success or not response_data:
                    logging.warning(f"‚ö†Ô∏è Failed to fetch products from {category_name} at offset {offset}")
                    consecutive_empty_pages += 1
                    offset += self.current_batch_size  # Move forward even on failure
                    continue
                
                search_results = response_data.get('searchProducts', {})
                products = search_results.get('products', [])
                
                if not products:
                    consecutive_empty_pages += 1
                    logging.info(f"‚ö†Ô∏è Empty page in {category_name} at offset {offset} ({consecutive_empty_pages}/{max_empty_pages})")
                    offset += self.current_batch_size
                    continue
                else:
                    consecutive_empty_pages = 0  # Reset counter on successful page
                
                # Process products
                new_products = []
                for product_data in products:
                    if product_data.get('id'):
                        product_id = product_data['id']
                        if product_id not in self.scraped_products:
                            self.scraped_products.add(product_id)
                            
                            # FIXED: Capture ALL product data instead of selective fields
                            formatted_product = {
                                "product": product_data,  # Complete product data with all fields
                                "scraped_from_category": category_name,
                                "scraped_at": get_amsterdam_time().isoformat(),
                                "optimization_version": "phase2b_complete_data"
                            }
                            new_products.append(formatted_product)
                
                if new_products:
                    self.save_products(new_products)
                    total_products += len(new_products)
                    self.total_scraped += len(new_products)
                
                # Check max_products limit if set
                if self.max_products and self.total_scraped >= self.max_products:
                    logging.info(f"üéØ Reached max_products limit: {self.max_products} (scraped: {self.total_scraped})")
                    break
                
                # DEEP PAGINATION: Use actual product count, not batch size
                products_in_batch = len(products)
                current_rate = products_in_batch / duration if duration > 0 else 0
                
                # Log every 100 products for deeper pagination visibility
                if offset % 1000 == 0 or products_in_batch > 0:
                    logging.info(f"‚ö° {category_name}: +{products_in_batch} products at offset {offset} ({current_rate:.1f} products/sec)")
                
                # Update progress with enhanced metrics
                progress_percent = min(100, (self.total_scraped / self.estimated_total_products) * 100)
                elapsed_time = time.time() - self.start_time
                overall_rate = self.total_scraped / elapsed_time if elapsed_time > 0 else 0
                
                update_progress('jumbo', 
                              progress_percent=progress_percent, 
                              products_scraped=self.total_scraped,
                              current_task=f"{category_name} (offset {offset}) - {overall_rate:.1f} products/sec")
                
                # Save progress more frequently for deep pagination
                if offset % 500 == 0:  # Every 500 products
                    self.save_progress(offset)
                
                # CRITICAL: Use actual products returned length, not batch size
                # This allows proper deep pagination beyond initial results
                offset += len(products) if len(products) > 0 else self.current_batch_size
                
                # OPTIMIZATION: Respectful delay with adaptive rate
                await asyncio.sleep(self.base_delay + uniform(0, 0.1))
            
            # Final save
            self.save_progress(offset)
            logging.info(f"‚úÖ Completed {category_name}: {total_products} products total (final offset: {offset})")
            
            return total_products

    async def run(self):
        """OPTIMIZED: Main scraping method with full catalog processing."""
        if self.scraping_completed:
            update_status('jumbo', ScraperStatus.COMPLETED, f"Already completed with {self.total_scraped} products")
            return
        
        update_status('jumbo', ScraperStatus.STARTING, "Initializing optimized GraphQL scraper")
        
        # Performance tracking
        scraping_start_time = time.time()
        
        connector = aiohttp.TCPConnector(limit=MAX_CONCURRENT_REQUESTS * 2, limit_per_host=MAX_CONCURRENT_REQUESTS)
        async with aiohttp.ClientSession(
            timeout=self.timeout_config,
            connector=connector,
            cookies=self.session_cookies
        ) as session:
            
            try:
                # Save session at start
                self.save_session(session)
                
                update_status('jumbo', ScraperStatus.RUNNING, "Fetching categories via GraphQL")
                
                # Fetch categories
                categories_query = self.get_categories_query()
                categories_data, duration, success = await self.make_graphql_request(session, categories_query)
                
                if not success or not categories_data:
                    raise Exception("Failed to fetch categories")
                
                categories = categories_data.get('searchProducts', {}).get('categoryTiles', [])
                logging.info(f"üéØ Found {len(categories)} categories to process")
                
                # FOCUS ON BBQ CATEGORY: This should return the full product catalog
                logging.info(f"üéØ Using BBQ category for full catalog access (deep pagination)")
                
                # Use only BBQ category which is known to return full catalog
                bbq_category = {"friendlyUrl": "bbq", "name": "BBQ (Full Catalog)"}
                
                update_status('jumbo', ScraperStatus.RUNNING, 
                             f"Deep scraping BBQ category with {self.current_batch_size}-product batches")
                
                # OPTIMIZATION: Focus all effort on deep BBQ pagination
                semaphore = asyncio.Semaphore(1)  # Single category, full focus
                
                logging.info(f"üöÄ Starting deep pagination of BBQ category...")
                total_bbq_products = await self.scrape_category(session, bbq_category, semaphore)
                
                logging.info(f"‚úÖ BBQ deep scraping complete: {total_bbq_products} products discovered")
                
                # Final performance report
                total_duration = time.time() - scraping_start_time
                final_rate = self.total_scraped / total_duration if total_duration > 0 else 0
                
                logging.info(f"üèÅ DEEP PAGINATION OPTIMIZATION RESULTS (Phase 2B):")
                logging.info(f"   Total products: {self.total_scraped}")
                logging.info(f"   Total time: {total_duration:.2f} seconds ({total_duration/60:.1f} minutes)")
                logging.info(f"   Final rate: {final_rate:.1f} products/second")
                logging.info(f"   Deep pagination: BBQ category with {self.current_batch_size}-product batches")
                logging.info(f"   Success rate: {self.successful_requests}/{self.successful_requests + self.failed_requests} ({self.successful_requests/(self.successful_requests + self.failed_requests)*100:.1f}%)")
                
                # Calculate improvement vs original
                original_rate = 2.2  # products per "page" in original scraper
                improvement_factor = final_rate / (original_rate / 30)  # Rough comparison
                logging.info(f"   üöÄ ESTIMATED IMPROVEMENT: {improvement_factor:.1f}x vs original scraper")
                
                # Mark completion
                with open(self.completed_flag, 'w') as f:
                    completion_data = {
                        'completed_at': get_amsterdam_time().isoformat(),
                        'total_products': self.total_scraped,
                        'duration_seconds': total_duration,
                        'products_per_second': final_rate,
                        'optimization_version': 'phase2b_deep_pagination',
                        'batch_size_used': self.current_batch_size,
                        'improvement_factor': improvement_factor,
                        'deep_pagination_method': 'bbq_category_focus',
                        'max_offset_reached': 'logged_in_category_method'
                    }
                    json.dump(completion_data, f, indent=4)
                
                update_status('jumbo', ScraperStatus.COMPLETED, 
                             f"Full catalog: {self.total_scraped} products in {total_duration:.1f}s ({final_rate:.1f} products/sec)")
                
            except Exception as e:
                logging.error(f"‚ùå Scraping failed: {e}")
                update_status('jumbo', ScraperStatus.FAILED, f"Error: {str(e)}")
                raise
            finally:
                self.save_progress()

async def main():
    """Main function to run the optimized Jumbo scraper."""
    logging.info("üöÄ Starting Jumbo GraphQL Optimized Scraper (Phase 2A)")
    
    scraper = JumboGraphQLOptimizedScraper()
    await scraper.run()
    
    logging.info("‚úÖ Scraper execution completed")

if __name__ == "__main__":
    asyncio.run(main())